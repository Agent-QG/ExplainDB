# Data Schema Overview

This document outlines the core data structures and flow used in the ExplainDB system.

## 1. Input: Natural Language Query

Users provide a natural language question along with a selected database. For example:

```
"Which 10 genres are the most popular by sales volume, and what is the average price per track in each?"
```

## 2. Processing Pipeline

The `process_query()` function returns the following structure:

| Key                 | Type         | Description                                      |
|---------------------|--------------|--------------------------------------------------|
| final_answer        | str          | Final natural language response to the query     |
| json_result         | List[Dict]   | Cleaned SQL result processed by GPT              |
| goals               | List[str]    | Exploration goals generated by LIDA              |
| chart_code_list     | List[str]    | Code snippets for chart generation               |
| chart_base64_list   | List[str]    | Base64-encoded PNG images of the charts          |
| explanation_list    | List[str]    | User-friendly explanations for the charts        |
| trace_log           | List[str]    | SQL reasoning process trace                      |
| df                  | DataFrame    | Final processed DataFrame                        |

## 3. Intermediate Structures

- DataFrame: Constructed from SQL result, refined via GPT, and used by LIDA for analysis.
- chart_code_list / chart_base64_list / explanation_list: Aligned by index and derived per goal.

## 4. Session History (Flask)

User history is tracked in a Flask session.

## 5. Supported Databases

Configured via SQLAlchemy URIs:

```python
DATABASES = {
    "Chinook": "sqlite:///Chinook.db",
    "Northwind": "postgresql+psycopg2://..."
}
```

## 6. Output Files

- `json_output/output.json`: JSON result after GPT processing
- `flask_session_data/`: Session state stored on disk

## Summary

ExplainDB processes natural language queries against structured databases using LLMs and visualization tools. It relies on session memory, in-memory DataFrames, and dynamic generation of charts and explanations for each user question.